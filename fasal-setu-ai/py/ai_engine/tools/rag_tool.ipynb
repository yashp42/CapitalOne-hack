{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b1b5f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n"
     ]
    }
   ],
   "source": [
    "print(\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51fbbdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installs (safe to re-run). Jupyter's %pip keeps the same interpreter.\n",
    "%pip install -qU pinecone python-dotenv langchain langchain-core \\\n",
    "               langchain-text-splitters langchain-pinecone tqdm pandas numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41b04571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: .env not found at e:\\capitalone\\CapitalOne-hack\\fasal-setu-ai\\py\\ai_engine\\tools\\.env\n",
      "\n",
      "=== RAG Tool Readiness ===\n",
      "• pinecone: 7.3.0\n",
      "• langchain: 0.3.27\n",
      "• langchain_text_splitters: not installed\n",
      "• langchain_pinecone: not installed\n",
      "• python-dotenv: not installed\n",
      "• ✅ PINECONE_API_KEY set\n",
      "• ✅ Pinecone location OK → region=us-east-1-aws, cloud=aws\n",
      "• ✅ PINECONE_INDEX = capitalone\n",
      "• ✅ NAMESPACE = default\n",
      "• ✅ EMBED_MODEL = llama-text-embed-v2\n",
      "• ✅ Data dir exists: e:\\capitalone\\CapitalOne-hack\\fasal-setu-ai\\py\\ai_engine\\tools\\ragdata\n",
      "• Found 2 .txt and 0 .json files.\n",
      "\n",
      "All good. Proceed to index & embedding setup.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Setup & Readiness ------------------------------------------------\n",
    "\n",
    "# Imports\n",
    "import os, sys, json, pathlib, importlib\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Repo roots & data dir\n",
    "ROOT = pathlib.Path.cwd()\n",
    "ENV_PATH = ROOT / \".env\"\n",
    "\n",
    "DATA_DIR = pathlib.Path(r\"e:/capitalone/CapitalOne-hack/fasal-setu-ai/py/ai_engine/tools/ragdata\")\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load .env\n",
    "if ENV_PATH.exists():\n",
    "    load_dotenv(ENV_PATH)\n",
    "else:\n",
    "    print(f\"WARNING: .env not found at {ENV_PATH}\")\n",
    "\n",
    "# Env vars (supports both legacy ENV and serverless region/cloud)\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV     = os.getenv(\"PINECONE_ENV\")                 # legacy (optional)\n",
    "PINECONE_CLOUD   = os.getenv(\"PINECONE_CLOUD\", \"aws\")        # serverless (optional)\n",
    "PINECONE_REGION  = os.getenv(\"PINECONE_REGION\", PINECONE_ENV or \"\")\n",
    "PINECONE_INDEX   = os.getenv(\"PINECONE_INDEX\", \"rag-llm1\")\n",
    "PINECONE_NS      = os.getenv(\"PINECONE_NAMESPACE\", \"default\")\n",
    "EMBED_MODEL      = os.getenv(\"EMBED_MODEL\", \"llama-text-embed-v2\")\n",
    "\n",
    "# Helper to get versions\n",
    "def _v(pkg):\n",
    "    try:\n",
    "        return importlib.import_module(pkg).__version__\n",
    "    except Exception:\n",
    "        return \"not installed\"\n",
    "\n",
    "# Readiness checks\n",
    "issues, checks = [], []\n",
    "\n",
    "# Package versions\n",
    "checks += [\n",
    "    f\"pinecone: {_v('pinecone')}\",\n",
    "    f\"langchain: {_v('langchain')}\",\n",
    "    f\"langchain_text_splitters: {_v('langchain_text_splitters')}\",\n",
    "    f\"langchain_pinecone: {_v('langchain_pinecone')}\",\n",
    "    f\"python-dotenv: {_v('dotenv')}\",\n",
    "]\n",
    "\n",
    "# Secrets\n",
    "if PINECONE_API_KEY:\n",
    "    checks.append(\"✅ PINECONE_API_KEY set\")\n",
    "else:\n",
    "    issues.append(\"❌ Set PINECONE_API_KEY in .env\")\n",
    "\n",
    "# Location (either legacy ENV or serverless region)\n",
    "if PINECONE_REGION:\n",
    "    checks.append(f\"✅ Pinecone location OK → region={PINECONE_REGION}, cloud={PINECONE_CLOUD}\")\n",
    "else:\n",
    "    issues.append(\"❌ Set PINECONE_REGION (and optional PINECONE_CLOUD) or legacy PINECONE_ENV\")\n",
    "\n",
    "checks += [\n",
    "    f\"✅ PINECONE_INDEX = {PINECONE_INDEX}\",\n",
    "    f\"✅ NAMESPACE = {PINECONE_NS}\",\n",
    "    f\"✅ EMBED_MODEL = {EMBED_MODEL}\",\n",
    "]\n",
    "\n",
    "# Data dir & file counts\n",
    "txts  = list(DATA_DIR.rglob(\"*.txt\"))\n",
    "jsons = list(DATA_DIR.rglob(\"*.json\"))\n",
    "checks.append(f\"✅ Data dir exists: {DATA_DIR}\")\n",
    "checks.append(f\"Found {len(txts)} .txt and {len(jsons)} .json files.\")\n",
    "\n",
    "# Pinecone SDK major version hint (recommend 7+)\n",
    "try:\n",
    "    import pinecone\n",
    "    pc_ver = pinecone.__version__\n",
    "    major = int(pc_ver.split(\".\")[0])\n",
    "    if major < 7:\n",
    "        issues.append(\"⚠️ pinecone SDK < 7.x detected — consider: %pip install -U pinecone\")\n",
    "except Exception:\n",
    "    issues.append(\"⚠️ Could not read pinecone version — ensure package installed\")\n",
    "\n",
    "print(\"\\n=== RAG Tool Readiness ===\")\n",
    "for c in checks: print(\"•\", c)\n",
    "if issues:\n",
    "    print(\"\\nIssues:\")\n",
    "    for i in issues: print(\" -\", i)\n",
    "else:\n",
    "    print(\"\\nAll good. Proceed to index & embedding setup.\")\n",
    "# ----------------------------------------------------------------------------- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba5e221b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hosted embedding ready: model=llama-text-embed-v2, dim=1024\n",
      "✅ Pinecone index 'capitalone' already exists.\n",
      "⚠️ Could not fetch index stats: 'NoneType' object is not callable\n",
      "Ready: Pinecone client + embeddings + index handle.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2 (fixed): Pinecone init + hosted embeddings + auto index creation --\n",
    "from typing import List, Dict, Any\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# 1) Init Pinecone client\n",
    "if not PINECONE_API_KEY:\n",
    "    raise RuntimeError(\"PINECONE_API_KEY missing. Add it to your .env and re-run Cell 1.\")\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# 2) Robust wrapper around Pinecone Hosted Embeddings (v7+ friendly)\n",
    "def _as_vectors(embed_out):\n",
    "    \"\"\"\n",
    "    Accepts either:\n",
    "      - pinecone.inference.EmbeddingsList (v7+)\n",
    "      - dict with 'data' (older shapes)\n",
    "      - list-like of rows\n",
    "    Returns: List[List[float]]\n",
    "    \"\"\"\n",
    "    # Prefer attribute `.data` if present (EmbeddingsList)\n",
    "    data = getattr(embed_out, \"data\", None)\n",
    "    if data is None:\n",
    "        # Maybe it's a dict with 'data'\n",
    "        if isinstance(embed_out, dict) and \"data\" in embed_out:\n",
    "            data = embed_out[\"data\"]\n",
    "        else:\n",
    "            # Otherwise assume it's already a list-like\n",
    "            data = embed_out\n",
    "\n",
    "    vectors = []\n",
    "    for row in data:\n",
    "        # v7 row objects expose `.values`; dicts expose ['values']\n",
    "        if hasattr(row, \"values\"):\n",
    "            vectors.append(row.values)\n",
    "        elif isinstance(row, dict) and \"values\" in row:\n",
    "            vectors.append(row[\"values\"])\n",
    "        else:\n",
    "            raise TypeError(f\"Unexpected embedding row type: {type(row)}\")\n",
    "    return vectors\n",
    "\n",
    "def _embed_with_pinecone(texts: List[str],\n",
    "                         model: str = EMBED_MODEL,\n",
    "                         input_type: str = \"passage\",\n",
    "                         truncate: str = \"END\") -> List[List[float]]:\n",
    "    if not texts:\n",
    "        return []\n",
    "    out = pc.inference.embed(\n",
    "        model=model,\n",
    "        inputs=texts,\n",
    "        parameters={\"input_type\": input_type, \"truncate\": truncate},\n",
    "    )\n",
    "    return _as_vectors(out)\n",
    "\n",
    "# 3) Infer embedding dimension (before creating the index)\n",
    "try:\n",
    "    _probe_vec = _embed_with_pinecone([\"dimension-probe\"])[0]\n",
    "    EMBED_DIM = len(_probe_vec)\n",
    "except Exception as e:\n",
    "    # Helpful debug if it ever breaks again\n",
    "    print(\"Embedding probe failed with type:\", type(e).__name__, \"→\", e)\n",
    "    raise RuntimeError(\n",
    "        f\"Failed to run hosted embeddings for model={EMBED_MODEL}. \"\n",
    "        f\"Check API key, model name, and account access.\"\n",
    "    )\n",
    "\n",
    "print(f\"✅ Hosted embedding ready: model={EMBED_MODEL}, dim={EMBED_DIM}\")\n",
    "\n",
    "# 4) Create index if missing (serverless)\n",
    "existing = {ix[\"name\"] for ix in pc.list_indexes()}\n",
    "if PINECONE_INDEX not in existing:\n",
    "    print(f\"ℹ️ Creating Pinecone index '{PINECONE_INDEX}' (cosine, dim={EMBED_DIM}) \"\n",
    "          f\"on {PINECONE_CLOUD}/{PINECONE_REGION} ...\")\n",
    "    pc.create_index(\n",
    "        name=PINECONE_INDEX,\n",
    "        dimension=EMBED_DIM,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=PINECONE_CLOUD, region=PINECONE_REGION),\n",
    "    )\n",
    "else:\n",
    "    print(f\"✅ Pinecone index '{PINECONE_INDEX}' already exists.\")\n",
    "\n",
    "# 5) Get an index handle\n",
    "index = pc.Index(PINECONE_INDEX)\n",
    "\n",
    "# 6) Small health check\n",
    "try:\n",
    "    stats = index.describe_index_stats()\n",
    "    print(\"✅ Index stats:\", {k: v for k, v in stats.items() if k in (\"dimension\", \"namespaces\", \"total_vector_count\")})\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Could not fetch index stats:\", e)\n",
    "\n",
    "# Public helpers\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    return _embed_with_pinecone(texts, model=EMBED_MODEL)\n",
    "\n",
    "def embedding_info() -> Dict[str, Any]:\n",
    "    return {\"model\": EMBED_MODEL, \"dimension\": EMBED_DIM, \"metric\": \"cosine\"}\n",
    "\n",
    "print(\"Ready: Pinecone client + embeddings + index handle.\")\n",
    "# -----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c5a5911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders & chunker ready. Next cell will embed + upsert in batches.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Loaders (.txt/.json) + robust chunking --------------------------\n",
    "from typing import Iterable, Dict, Any, List, Tuple\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _hash(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()[:12]\n",
    "\n",
    "def _norm_source(path: pathlib.Path, extra: str = \"\") -> str:\n",
    "    rel = path.relative_to(ROOT)\n",
    "    return f\"{rel.as_posix()}{('::' + extra) if extra else ''}\"\n",
    "\n",
    "# ---------- JSON flattening ----------\n",
    "def _flatten_json(obj: Any, prefix: str = \"\") -> Iterable[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Yields (path, text_value) pairs.\n",
    "    - Dicts → keys appended with '/'.\n",
    "    - Lists → indices appended with '[i]'.\n",
    "    - Scalars → returned as strings.\n",
    "    Non-text scalars are stringified.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            newp = f\"{prefix}/{k}\" if prefix else k\n",
    "            yield from _flatten_json(v, newp)\n",
    "    elif isinstance(obj, list):\n",
    "        for i, v in enumerate(obj):\n",
    "            newp = f\"{prefix}[{i}]\" if prefix else f\"[{i}]\"\n",
    "            yield from _flatten_json(v, newp)\n",
    "    else:\n",
    "        # scalar\n",
    "        text = \"\" if obj is None else str(obj)\n",
    "        if text.strip():\n",
    "            yield prefix, text\n",
    "\n",
    "# ---------- corpus loading ----------\n",
    "def load_corpus(data_dir: pathlib.Path = DATA_DIR) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Returns a list of raw docs:\n",
    "    {\n",
    "      'doc_id': str,                  # stable id seed (before chunking)\n",
    "      'text': str,                    # full text (or json slice)\n",
    "      'source_stamp': str,            # e.g., py/ai_engine/tools/data/foo.json::crops[2]/name\n",
    "      'meta': {'path': str, 'kind': 'txt'|'json', 'json_path': str|None}\n",
    "    }\n",
    "    \"\"\"\n",
    "    docs: List[Dict[str, Any]] = []\n",
    "\n",
    "    # .txt files\n",
    "    for path in data_dir.rglob(\"*.txt\"):\n",
    "        try:\n",
    "            text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            text = path.read_text(errors=\"ignore\")\n",
    "        src = _norm_source(path)\n",
    "        if text.strip():\n",
    "            docs.append({\n",
    "                \"doc_id\": _hash(src),\n",
    "                \"text\": text,\n",
    "                \"source_stamp\": src,\n",
    "                \"meta\": {\"path\": src, \"kind\": \"txt\", \"json_path\": None}\n",
    "            })\n",
    "\n",
    "    # .json files\n",
    "    for path in data_dir.rglob(\"*.json\"):\n",
    "        try:\n",
    "            obj = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to parse JSON: {path}: {e}\")\n",
    "            continue\n",
    "        base = _norm_source(path)\n",
    "        for jpath, text in _flatten_json(obj):\n",
    "            src = _norm_source(path, jpath)\n",
    "            docs.append({\n",
    "                \"doc_id\": _hash(src),\n",
    "                \"text\": text,\n",
    "                \"source_stamp\": src,\n",
    "                \"meta\": {\"path\": base, \"kind\": \"json\", \"json_path\": jpath}\n",
    "            })\n",
    "\n",
    "    print(f\"Loaded {len(docs)} raw doc items \"\n",
    "          f\"({sum(1 for d in docs if d['meta']['kind']=='txt')} txt, \"\n",
    "          f\"{sum(1 for d in docs if d['meta']['kind']=='json')} json-slices).\")\n",
    "    return docs\n",
    "\n",
    "# ---------- chunking ----------\n",
    "DEFAULT_CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", \"1000\"))\n",
    "DEFAULT_CHUNK_OVERLAP = int(os.getenv(\"CHUNK_OVERLAP\", \"120\"))\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=DEFAULT_CHUNK_SIZE,\n",
    "    chunk_overlap=DEFAULT_CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "def chunk_documents(raw_docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Splits raw docs into chunk records:\n",
    "    {\n",
    "      'id': str,               # unique per chunk\n",
    "      'text': str,\n",
    "      'source_stamp': str,     # preserved\n",
    "      'metadata': {...}        # ready for Pinecone\n",
    "    }\n",
    "    \"\"\"\n",
    "    chunks: List[Dict[str, Any]] = []\n",
    "    for rd in raw_docs:\n",
    "        parts = splitter.split_text(rd[\"text\"])\n",
    "        for idx, part in enumerate(parts):\n",
    "            if not part.strip():\n",
    "                continue\n",
    "            cid_seed = f\"{rd['doc_id']}::{idx}\"\n",
    "            cid = _hash(cid_seed)\n",
    "            meta = {\n",
    "                \"source_stamp\": rd[\"source_stamp\"],\n",
    "                \"path\": rd[\"meta\"][\"path\"],\n",
    "                \"kind\": rd[\"meta\"][\"kind\"],\n",
    "                \"json_path\": rd[\"meta\"][\"json_path\"],\n",
    "                \"chunk_index\": idx,\n",
    "                \"chunk_size\": DEFAULT_CHUNK_SIZE,\n",
    "                \"chunk_overlap\": DEFAULT_CHUNK_OVERLAP,\n",
    "                \"embed_model\": EMBED_MODEL,\n",
    "            }\n",
    "            chunks.append({\n",
    "                \"id\": cid,\n",
    "                \"text\": part,\n",
    "                \"source_stamp\": rd[\"source_stamp\"],\n",
    "                \"metadata\": meta,\n",
    "            })\n",
    "    print(f\"Chunked into {len(chunks)} chunks \"\n",
    "          f\"(size≈{DEFAULT_CHUNK_SIZE}, overlap={DEFAULT_CHUNK_OVERLAP}).\")\n",
    "    return chunks\n",
    "\n",
    "# Quick preview utility (optional)\n",
    "def preview_chunks(chunks: List[Dict[str, Any]], n: int = 5):\n",
    "    rows = [{\n",
    "        \"id\": c[\"id\"],\n",
    "        \"len\": len(c[\"text\"]),\n",
    "        \"source_stamp\": c[\"source_stamp\"],\n",
    "        \"chunk_index\": c[\"metadata\"][\"chunk_index\"],\n",
    "        \"kind\": c[\"metadata\"][\"kind\"],\n",
    "    } for c in chunks[:n]]\n",
    "    try:\n",
    "        display(pd.DataFrame(rows))\n",
    "    except Exception:\n",
    "        print(rows)\n",
    "\n",
    "print(\"Loaders & chunker ready. Next cell will embed + upsert in batches.\")\n",
    "# -----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da3888b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed & upsert utilities ready. Next cell will add the search API + LangChain tool wrapper.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 4: Embed & upsert to Pinecone (batched) + rebuild helpers ----------\n",
    "from typing import List, Dict, Any, Iterable, Optional\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import traceback\n",
    "\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"64\"))\n",
    "UPSERT_NAMESPACE = PINECONE_NS\n",
    "\n",
    "def _batched(iterable: Iterable[Any], n: int) -> Iterable[List[Any]]:\n",
    "    batch = []\n",
    "    for x in iterable:\n",
    "        batch.append(x)\n",
    "        if len(batch) >= n:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "def _prepare_vectors(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Turn chunk records into Pinecone vectors with embeddings.\"\"\"\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    embs = embed_texts(texts)\n",
    "    vectors = []\n",
    "    for c, v in zip(chunks, embs):\n",
    "        # store text + source + small meta for downstream retrieval\n",
    "        meta = dict(c[\"metadata\"])\n",
    "        meta.update({\n",
    "            \"text\": c[\"text\"],\n",
    "            \"source_stamp\": c[\"source_stamp\"],\n",
    "        })\n",
    "        vectors.append({\n",
    "            \"id\": c[\"id\"],\n",
    "            \"values\": v,\n",
    "            \"metadata\": meta,\n",
    "        })\n",
    "    return vectors\n",
    "\n",
    "def upsert_chunks(chunks: List[Dict[str, Any]],\n",
    "                  index_handle,\n",
    "                  namespace: Optional[str] = None,\n",
    "                  batch_size: int = BATCH_SIZE,\n",
    "                  max_retries: int = 5) -> int:\n",
    "    \"\"\"Embed and upsert chunks with retry/backoff. Returns total upserted.\"\"\"\n",
    "    total = 0\n",
    "    namespace = namespace or UPSERT_NAMESPACE\n",
    "    for batch in tqdm(list(_batched(chunks, batch_size)), desc=\"Upserting\"):\n",
    "        # 1) embed\n",
    "        vectors = _prepare_vectors(batch)\n",
    "\n",
    "        # 2) upsert with retries\n",
    "        attempt, backoff = 0, 1.0\n",
    "        while True:\n",
    "            try:\n",
    "                index_handle.upsert(vectors=vectors, namespace=namespace)\n",
    "                total += len(vectors)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                if attempt > max_retries:\n",
    "                    print(\"❌ Upsert failed after retries. Last error:\\n\", e)\n",
    "                    traceback.print_exc(limit=1)\n",
    "                    break\n",
    "                time.sleep(backoff)\n",
    "                backoff *= 2\n",
    "    return total\n",
    "\n",
    "def build_index(data_dir: pathlib.Path = DATA_DIR,\n",
    "                namespace: Optional[str] = None,\n",
    "                batch_size: int = BATCH_SIZE) -> Dict[str, Any]:\n",
    "    \"\"\"Full pipeline: load → chunk → embed+upsert. Returns summary.\"\"\"\n",
    "    ns = namespace or UPSERT_NAMESPACE\n",
    "    print(f\"Building index '{PINECONE_INDEX}' namespace='{ns}' from: {data_dir}\")\n",
    "\n",
    "    raw = load_corpus(data_dir)\n",
    "    chunks = chunk_documents(raw)\n",
    "    count = upsert_chunks(chunks, index, namespace=ns, batch_size=batch_size)\n",
    "\n",
    "    summary = {\n",
    "        \"index\": PINECONE_INDEX,\n",
    "        \"namespace\": ns,\n",
    "        \"files_seen\": len({d['meta']['path'] for d in raw}),\n",
    "        \"raw_items\": len(raw),\n",
    "        \"chunks_upserted\": count,\n",
    "        \"embed_model\": EMBED_MODEL,\n",
    "        \"dim\": embedding_info()[\"dimension\"],\n",
    "    }\n",
    "    print(\"✅ Build complete:\", summary)\n",
    "    return summary\n",
    "\n",
    "def wipe_namespace(namespace: Optional[str] = None):\n",
    "    \"\"\"Deletes all vectors in a namespace (careful!).\"\"\"\n",
    "    ns = namespace or UPSERT_NAMESPACE\n",
    "    confirm = True  # flip to a prompt in interactive use if you prefer\n",
    "    if confirm:\n",
    "        print(f\"⚠️ Deleting all vectors in index='{PINECONE_INDEX}', namespace='{ns}' ...\")\n",
    "        index.delete(delete_all=True, namespace=ns)\n",
    "        print(\"✅ Namespace wiped.\")\n",
    "\n",
    "print(\"Embed & upsert utilities ready. Next cell will add the search API + LangChain tool wrapper.\")\n",
    "# -----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35f690f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Patched _prepare_vectors with Pinecone-safe metadata (nulls removed).\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 4a: Pinecone metadata sanitizer (fix nulls like json_path=None) ---\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "def _pc_clean_meta(meta: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    clean: Dict[str, Any] = {}\n",
    "    for k, v in meta.items():\n",
    "        if v is None:\n",
    "            # Drop nulls entirely (or set \"\" if you prefer)\n",
    "            continue\n",
    "        if isinstance(v, (str, int, float, bool)):\n",
    "            clean[k] = v\n",
    "        elif isinstance(v, list):\n",
    "            # Pinecone allows list of strings — coerce items to str, skip None\n",
    "            clean[k] = [str(x) for x in v if x is not None]\n",
    "        else:\n",
    "            # Fallback: stringify complex types\n",
    "            clean[k] = str(v)\n",
    "    return clean\n",
    "\n",
    "# Override _prepare_vectors to use the sanitizer\n",
    "def _prepare_vectors(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    embs = embed_texts(texts)\n",
    "    vectors = []\n",
    "    for c, v in zip(chunks, embs):\n",
    "        meta = dict(c[\"metadata\"])\n",
    "        meta.update({\n",
    "            \"text\": c[\"text\"],\n",
    "            \"source_stamp\": c[\"source_stamp\"],\n",
    "        })\n",
    "        meta = _pc_clean_meta(meta)\n",
    "        vectors.append({\n",
    "            \"id\": c[\"id\"],\n",
    "            \"values\": v,\n",
    "            \"metadata\": meta,\n",
    "        })\n",
    "    return vectors\n",
    "\n",
    "print(\"✅ Patched _prepare_vectors with Pinecone-safe metadata (nulls removed).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03ddf587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search API ready. Next cell will show an end-to-end demo: build → search.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 5: Search (top-k) + LangChain tool wrapper -------------------------\n",
    "from typing import Dict, Any, List, Optional, Union\n",
    "from langchain.tools import Tool\n",
    "\n",
    "DEFAULT_TOP_K = int(os.getenv(\"TOP_K\", \"5\"))\n",
    "\n",
    "def _normalize_match(m: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Pinecone v7 .query() returns matches with fields:\n",
    "      id, score, metadata (and maybe values if requested).\n",
    "    We map to the contract: {text, source_stamp, score, id}\n",
    "    \"\"\"\n",
    "    meta = m.get(\"metadata\", {}) or {}\n",
    "    return {\n",
    "        \"id\": m.get(\"id\"),\n",
    "        \"score\": m.get(\"score\"),\n",
    "        \"text\": meta.get(\"text\", \"\"),\n",
    "        \"source_stamp\": meta.get(\"source_stamp\", meta.get(\"path\", \"\")),\n",
    "    }\n",
    "\n",
    "def semantic_search(query: str,\n",
    "                    top_k: int = DEFAULT_TOP_K,\n",
    "                    namespace: Optional[str] = None,\n",
    "                    metadata_filter: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Runs an embedding search in Pinecone and returns normalized passages.\n",
    "    \"\"\"\n",
    "    if not query or not query.strip():\n",
    "        return []\n",
    "\n",
    "    ns = namespace or UPSERT_NAMESPACE\n",
    "    vec = embed_texts([query])[0]\n",
    "\n",
    "    res = index.query(\n",
    "        namespace=ns,\n",
    "        vector=vec,\n",
    "        top_k=top_k,\n",
    "        include_values=False,\n",
    "        include_metadata=True,\n",
    "        filter=metadata_filter or None,\n",
    "    )\n",
    "\n",
    "    matches = res.get(\"matches\", []) if isinstance(res, dict) else getattr(res, \"matches\", [])\n",
    "    return [_normalize_match(m) for m in matches]\n",
    "\n",
    "def rag_search(args: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    LangChain-tool-compatible callable.\n",
    "    Accepts either:\n",
    "      - a plain query string, or\n",
    "      - a dict with keys:\n",
    "          query (str, required),\n",
    "          top_k (int, optional),\n",
    "          namespace (str, optional),\n",
    "          filter (dict, optional metadata filter)\n",
    "    Returns:\n",
    "      {\"data\": [{\"text\":..., \"source_stamp\":..., \"score\":..., \"id\":...}, ...]}\n",
    "    \"\"\"\n",
    "    if isinstance(args, str):\n",
    "        query, top_k, ns, filt = args, DEFAULT_TOP_K, UPSERT_NAMESPACE, None\n",
    "    else:\n",
    "        query = args.get(\"query\", \"\")\n",
    "        top_k = int(args.get(\"top_k\", DEFAULT_TOP_K))\n",
    "        ns    = args.get(\"namespace\", UPSERT_NAMESPACE)\n",
    "        filt  = args.get(\"filter\")\n",
    "\n",
    "    results = semantic_search(query=query, top_k=top_k, namespace=ns, metadata_filter=filt)\n",
    "    return {\"data\": results}\n",
    "\n",
    "# Register as a LangChain Tool (ready for pipeline use)\n",
    "rag_search_tool = Tool(\n",
    "    name=\"rag_search\",\n",
    "    description=(\n",
    "        \"Semantic search over the Pinecone index. \"\n",
    "        \"Args can be a string query or a JSON dict with keys \"\n",
    "        \"{query, top_k?, namespace?, filter?}. Returns passages \"\n",
    "        \"as {data: [{text, source_stamp, score, id}, ...]}.\"\n",
    "    ),\n",
    "    func=rag_search,\n",
    ")\n",
    "\n",
    "print(\"Search API ready. Next cell will show an end-to-end demo: build → search.\")\n",
    "# -----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87c5c138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MMR reranker ready. Use rag_search({... 'rerank': True, 'fetch_k': 20, 'lambda_mult': 0.7}).\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 5a: MMR reranker + rag_search(rerank=...) --------------------------\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "import numpy as np\n",
    "\n",
    "def _cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def _mmr_rerank(query_vec: np.ndarray,\n",
    "                cand_vecs: np.ndarray,\n",
    "                lambda_mult: float = 0.7,\n",
    "                top_k: int = 5) -> List[int]:\n",
    "    \"\"\"\n",
    "    Classic MMR (Carbonell & Goldstein, 1998):\n",
    "    Select items that maximize: λ * sim(candidate, query) - (1-λ) * max_sim(candidate, selected)\n",
    "    Returns indices of selected candidates in order.\n",
    "    \"\"\"\n",
    "    n = cand_vecs.shape[0]\n",
    "    if n == 0:\n",
    "        return []\n",
    "    top_k = min(top_k, n)\n",
    "\n",
    "    # Precompute similarities to query\n",
    "    qnorm = np.linalg.norm(query_vec)\n",
    "    cnorms = np.linalg.norm(cand_vecs, axis=1)\n",
    "    # Avoid division by zero\n",
    "    qnorm = 1e-9 if qnorm == 0 else qnorm\n",
    "    cnorms = np.where(cnorms == 0, 1e-9, cnorms)\n",
    "    sim_to_query = (cand_vecs @ query_vec) / (cnorms * qnorm)\n",
    "\n",
    "    selected: List[int] = []\n",
    "    remaining = set(range(n))\n",
    "\n",
    "    while len(selected) < top_k and remaining:\n",
    "        if not selected:\n",
    "            # pick the best by query similarity\n",
    "            i = int(np.argmax(sim_to_query))\n",
    "            selected.append(i)\n",
    "            remaining.remove(i)\n",
    "            continue\n",
    "\n",
    "        # compute diversity term: for each candidate, its max similarity to any already selected\n",
    "        sel_vecs = cand_vecs[selected]\n",
    "        # cosine sim with each selected, take max along selected axis\n",
    "        sims_matrix = (cand_vecs @ sel_vecs.T) / (cnorms[:, None] * np.linalg.norm(sel_vecs, axis=1)[None, :])\n",
    "        max_sim_to_selected = np.max(sims_matrix, axis=1)\n",
    "\n",
    "        # score = λ * sim(q) - (1-λ) * max_sim(selected)\n",
    "        mmr_scores = lambda_mult * sim_to_query - (1.0 - lambda_mult) * max_sim_to_selected\n",
    "\n",
    "        # ignore already selected by setting to -inf\n",
    "        mmr_scores[list(selected)] = -np.inf\n",
    "        # pick the best remaining\n",
    "        i = int(np.argmax(mmr_scores))\n",
    "        if i in remaining:\n",
    "            selected.append(i)\n",
    "            remaining.remove(i)\n",
    "        else:\n",
    "            # fallback: pick any remaining with highest sim_to_query\n",
    "            i = max(list(remaining), key=lambda j: sim_to_query[j])\n",
    "            selected.append(i)\n",
    "            remaining.remove(i)\n",
    "\n",
    "    return selected\n",
    "\n",
    "def semantic_search_reranked(query: str,\n",
    "                             top_k: int = DEFAULT_TOP_K,\n",
    "                             namespace: Optional[str] = None,\n",
    "                             metadata_filter: Optional[Dict[str, Any]] = None,\n",
    "                             fetch_k: Optional[int] = None,\n",
    "                             lambda_mult: float = 0.7) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    1) Query Pinecone for a larger candidate set (fetch_k).\n",
    "    2) Embed the query + candidate texts with the hosted model.\n",
    "    3) Apply MMR to promote relevance and diversity.\n",
    "    4) Return top_k passages in the new order.\n",
    "    \"\"\"\n",
    "    if not query or not query.strip():\n",
    "        return []\n",
    "\n",
    "    ns = namespace or UPSERT_NAMESPACE\n",
    "    fetch_k = fetch_k or max(top_k * 3, top_k)\n",
    "\n",
    "    # (a) Initial ANN retrieve\n",
    "    q_vec = embed_texts([query])[0]\n",
    "    res = index.query(\n",
    "        namespace=ns,\n",
    "        vector=q_vec,\n",
    "        top_k=fetch_k,\n",
    "        include_values=False,\n",
    "        include_metadata=True,\n",
    "        filter=metadata_filter or None,\n",
    "    )\n",
    "    matches = res.get(\"matches\", []) if isinstance(res, dict) else getattr(res, \"matches\", [])\n",
    "    if not matches:\n",
    "        return []\n",
    "\n",
    "    # (b) Embed candidate texts (use the text stored in metadata)\n",
    "    cand_texts = [ (m.get(\"metadata\", {}) or {}).get(\"text\", \"\") for m in matches ]\n",
    "    # guard empty strings\n",
    "    pairs = [(i, t) for i, t in enumerate(cand_texts) if t.strip()]\n",
    "    if not pairs:\n",
    "        return []\n",
    "\n",
    "    idxs, texts = zip(*pairs)\n",
    "    cand_vecs = np.array(embed_texts(list(texts)), dtype=np.float32)\n",
    "\n",
    "    # (c) Run MMR on the subset with text; map back to original match indices\n",
    "    selected_local = _mmr_rerank(np.array(q_vec, dtype=np.float32), cand_vecs,\n",
    "                                 lambda_mult=lambda_mult, top_k=min(top_k, len(idxs)))\n",
    "    selected_global = [idxs[i] for i in selected_local]\n",
    "\n",
    "    # (d) Normalize to the desired output schema\n",
    "    def _norm(m):\n",
    "        meta = m.get(\"metadata\", {}) or {}\n",
    "        return {\n",
    "            \"id\": m.get(\"id\"),\n",
    "            \"score\": m.get(\"score\"),\n",
    "            \"text\": meta.get(\"text\", \"\"),\n",
    "            \"source_stamp\": meta.get(\"source_stamp\", meta.get(\"path\", \"\")),\n",
    "        }\n",
    "\n",
    "    return [_norm(matches[i]) for i in selected_global]\n",
    "\n",
    "# Upgrade rag_search to accept rerank\n",
    "def rag_search(args: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Accepts string or dict.\n",
    "    Dict keys:\n",
    "      query (str, required)\n",
    "      top_k (int, optional)\n",
    "      namespace (str, optional)\n",
    "      filter (dict, optional)\n",
    "      rerank (bool, optional)           # <--- new\n",
    "      fetch_k (int, optional)           # <--- candidate pool for rerank\n",
    "      lambda_mult (float, optional)     # <--- MMR lambda (0..1), higher = more relevance\n",
    "    \"\"\"\n",
    "    if isinstance(args, str):\n",
    "        query = args\n",
    "        opts = {}\n",
    "    else:\n",
    "        query = args.get(\"query\", \"\")\n",
    "        opts = args\n",
    "\n",
    "    top_k       = int(opts.get(\"top_k\", DEFAULT_TOP_K))\n",
    "    ns          = opts.get(\"namespace\", UPSERT_NAMESPACE)\n",
    "    filt        = opts.get(\"filter\")\n",
    "    rerank      = bool(opts.get(\"rerank\", False))\n",
    "    fetch_k     = opts.get(\"fetch_k\")\n",
    "    lambda_mult = float(opts.get(\"lambda_mult\", 0.7))\n",
    "\n",
    "    if rerank:\n",
    "        data = semantic_search_reranked(query=query, top_k=top_k, namespace=ns,\n",
    "                                        metadata_filter=filt, fetch_k=fetch_k,\n",
    "                                        lambda_mult=lambda_mult)\n",
    "    else:\n",
    "        data = semantic_search(query=query, top_k=top_k, namespace=ns, metadata_filter=filt)\n",
    "\n",
    "    return {\"data\": data}\n",
    "\n",
    "print(\"✅ MMR reranker ready. Use rag_search({... 'rerank': True, 'fetch_k': 20, 'lambda_mult': 0.7}).\")\n",
    "# -----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85646f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1961f91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: e:\\capitalone\\CapitalOne-hack\\fasal-setu-ai\\py\\ai_engine\\tools\\ragdata\n",
      "Index: capitalone | Namespace: default\n",
      "Embedding: {'model': 'llama-text-embed-v2', 'dimension': 1024, 'metric': 'cosine'}\n",
      "Building index 'capitalone' namespace='default' from: e:\\capitalone\\CapitalOne-hack\\fasal-setu-ai\\py\\ai_engine\\tools\\ragdata\n",
      "Loaded 2 raw doc items (2 txt, 0 json-slices).\n",
      "Chunked into 12 chunks (size≈1000, overlap=120).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserting: 100%|██████████| 1/1 [00:03<00:00,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Build complete: {'index': 'capitalone', 'namespace': 'default', 'files_seen': 2, 'raw_items': 2, 'chunks_upserted': 12, 'embed_model': 'llama-text-embed-v2', 'dim': 1024}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'index': 'capitalone',\n",
       " 'namespace': 'default',\n",
       " 'files_seen': 2,\n",
       " 'raw_items': 2,\n",
       " 'chunks_upserted': 12,\n",
       " 'embed_model': 'llama-text-embed-v2',\n",
       " 'dim': 1024}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Cell 6: Demo — build index then run a few searches ----------------------\n",
    "# If you haven’t added files yet, drop .txt/.json into DATA_DIR first, then re-run.\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"Index:\", PINECONE_INDEX, \"| Namespace:\", UPSERT_NAMESPACE)\n",
    "print(\"Embedding:\", embedding_info())\n",
    "\n",
    "# 1) Build (load → chunk → embed → upsert)\n",
    "summary = build_index(DATA_DIR, namespace=UPSERT_NAMESPACE)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef6c0adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pretty print for rag_search results (fixes NameError: textwrap) ---------\n",
    "import textwrap\n",
    "\n",
    "def show_results(results, n=5, width=300):\n",
    "    data = results.get(\"data\", []) if isinstance(results, dict) else results\n",
    "    if not data:\n",
    "        print(\"No results.\")\n",
    "        return\n",
    "    for i, r in enumerate(data[:n], 1):\n",
    "        score = r.get(\"score\")\n",
    "        score_str = f\"{score:.4f}\" if isinstance(score, (int, float)) else str(score)\n",
    "        txt = r.get(\"text\", \"\") or \"\"\n",
    "        oneline = \" \".join(txt.split())\n",
    "        snippet = textwrap.shorten(oneline, width=width, placeholder=\"...\")\n",
    "        print(f\"\\n#{i}  score={score_str}\\nsource={r.get('source_stamp')}\\ntext  : {snippet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55342369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1  score=0.3564\n",
      "source=ragdata/cropinfogondhiyaICAR.txt\n",
      "text  : Livestock and fodder measures: Reserve fodder from crop residue; grow sorghum, bajra, maize for green fodder; fodder banks at village level; harvest top fodder from trees (Leucaena, Glyricidia); urea molasses treatment of straw.\n",
      "\n",
      "#2  score=0.3396\n",
      "source=ragdata/ICAR_CRIDA_DOC_Bihar.txt\n",
      "text  : 1.1 Agro-Climatic/Ecological Zone Satpura range and Wainganga Valley, hot moist subhumid ESR with shallow to deep loamy to clayey mixed Red and Black soils, low to medium AWC and LGP 180-210 days. 1.7 Area under major field crops Kharif/Rabi/Summer: Paddy 190.9ha, Soybean 9.2ha, Pigeonpea 4.2ha,...\n",
      "\n",
      "#3  score=0.3138\n",
      "source=ragdata/ICAR_CRIDA_DOC_Bihar.txt\n",
      "text  : 1.1 Agro-climatic zone Eastern Vidarbha zone, hot sub-humid, dry; soils deep black (~79%), medium black (~8%), shallow black (~13%). Main Kharif crops: Paddy (~205,000ha), pigeon pea, sesame. Sowing windows: Kharif paddy (rainfed) early July, irrigated early June, Gram/Wheat mid-Nov to mid-Dec....\n",
      "\n",
      "#4  score=0.3003\n",
      "source=ragdata/cropinfogondhiyaICAR.txt\n",
      "text  : Medium land (120–140 days): MTU 1156, CR Dhan 310, Sampada, Lalat, Nua acharmati. Lowland (>140 days): Swarna, Pooja, Durga, Pratikshya, etc. Cultural practices: Seed treatment with fungicides before sowing Wet or dry nursery preparation; recommended spacing 20 × 15 cm; basal fertilizer dose (DAP...\n",
      "\n",
      "#5  score=0.2930\n",
      "source=ragdata/cropinfogondhiyaICAR.txt\n",
      "text  : Soils: Deep black (~79%), medium black (~8%), shallow black (~13%). Net sown area: ~183,000 ha; ~54% irrigated (mainly canals). Main Kharif crops: Paddy (~205,000 ha total), pigeon pea, sesame. Rabi crops: Wheat, gram, linseed. Sowing windows: Kharif paddy (rainfed): Early July; irrigated: Early...\n"
     ]
    }
   ],
   "source": [
    "res = rag_search({\n",
    "    \"query\": \"maize in gondhiya\",\n",
    "    \"top_k\": 5,\n",
    "    \"rerank\": True,\n",
    "    \"fetch_k\": 24,\n",
    "    \"lambda_mult\": 0.7\n",
    "})\n",
    "show_results(res, n=5, width=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "189e91b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.delete(delete_all=True, namespace=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a6360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
